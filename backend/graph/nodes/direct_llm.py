"""å¸¸è§„LLMèŠ‚ç‚¹ - ç›´æ¥å¯¹è¯æ¨¡å¼"""

from typing import Dict, Any
from graph.state import GraphState
from langchain_community.chat_models import ChatTongyi
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from app.core.settings import settings


async def direct_llm_node(state: GraphState) -> Dict[str, Any]:
    """
    å¸¸è§„LLMèŠ‚ç‚¹ï¼šç®€å•çš„ç›´æ¥å¯¹è¯ï¼Œä¸ä½¿ç”¨å·¥å…·
    é€‚ç”¨äºæ—¥å¸¸èŠå¤©ã€ç®€å•é—®ç­”

    Args:
        state: å½“å‰çŠ¶æ€

    Returns:
        Dict: æ›´æ–°åçš„çŠ¶æ€ï¼ŒåŒ…å« final_answer
    """
    query = state.get("query", "")
    chat_history = state.get("chat_history", [])

    print(f"\n[ğŸ’¬ Direct LLM] å¤„ç†æŸ¥è¯¢: {query[:50]}...")

    try:
        # åˆ›å»ºLLM
        llm = ChatTongyi(
            model="qwen-plus-2025-07-28",
            temperature=0.2,
            dashscope_api_key=settings.DASHSCOPE_API_KEY or "",
        )

        # æ„å»ºprompt
        system_prompt = """ä½ æ˜¯ä¸­æ–‡åŠ©ç†ï¼Œè¯·ç”¨ç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›ç­”
2. ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„è¯­æ°”
3. å¦‚æœä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯šå®å‘ŠçŸ¥
4. é¿å…é‡å¤å’Œå•°å—¦
"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history", optional=True),
            ("human", "{query}")
        ])

        # è½¬æ¢èŠå¤©å†å²æ ¼å¼
        messages = []
        for role, content in chat_history:
            if role == "human":
                messages.append(("human", content))
            elif role == "ai":
                messages.append(("ai", content))

        # è°ƒç”¨LLM
        chain = prompt | llm
        response = await chain.ainvoke({
            "query": query,
            "chat_history": messages if messages else None
        })

        answer = response.content if hasattr(response, 'content') else str(response)

        print(f"[ğŸ’¬ Direct LLM] ç”Ÿæˆç­”æ¡ˆ: {answer[:100]}...")

        return {
            **state,
            "final_answer": answer,
            "output": answer,
        }

    except Exception as e:
        print(f"[âŒ Direct LLM] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

        return {
            **state,
            "final_answer": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºé”™ï¼š{str(e)}",
            "output": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºé”™ï¼š{str(e)}",
            "error": str(e),
        }
