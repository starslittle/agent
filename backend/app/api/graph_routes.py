"""Graph API è·¯ç”± - ç»Ÿä¸€æµå¼è¾“å‡ºæ¥å£"""

from typing import Dict, Any
from pathlib import Path

import yaml
from fastapi import HTTPException
from sse_starlette.sse import EventSourceResponse

from app.api.intent_router import classify_and_route
from app.api.agent_factory import create_agent_from_config
from graph import run_graph


_STREAM_AGENT_CACHE: dict[str, object] = {}
_STREAM_DEFAULT: str | None = None


def _resolve_agents_yaml() -> Path:
    backend_root = Path(__file__).resolve().parents[2]
    repo_root = backend_root.parent
    candidates = [
        backend_root / "configs" / "agents.yaml",
        backend_root / "agents.yaml",
        repo_root / "configs" / "agents.yaml",
        repo_root / "backend" / "configs" / "agents.yaml",
        repo_root / "agents.yaml",
    ]
    for p in candidates:
        try:
            if p.exists():
                return p
        except Exception:
            continue
    tried = " | ".join(str(x) for x in candidates)
    raise FileNotFoundError(f"agents.yaml not found; tried: {tried}")


def _load_stream_agents() -> None:
    global _STREAM_AGENT_CACHE, _STREAM_DEFAULT
    if _STREAM_AGENT_CACHE:
        return
    yaml_path = _resolve_agents_yaml()
    data = yaml.safe_load(yaml_path.read_text(encoding="utf-8"))
    _STREAM_AGENT_CACHE = {}
    _STREAM_DEFAULT = None
    for agent_cfg in data.get("agents", []):
        name = agent_cfg.get("name")
        conf = agent_cfg.get("config", {})
        if not name:
            continue
        _STREAM_AGENT_CACHE[name] = create_agent_from_config(conf, streaming_override=True)
        if agent_cfg.get("is_default"):
            _STREAM_DEFAULT = name
    if _STREAM_DEFAULT is None and _STREAM_AGENT_CACHE:
        _STREAM_DEFAULT = list(_STREAM_AGENT_CACHE.keys())[0]


def _get_stream_executor(agent_name: str | None):
    _load_stream_agents()
    name = agent_name or _STREAM_DEFAULT
    if not name or name not in _STREAM_AGENT_CACHE:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°æŒ‡å®š agent: {agent_name}")
    return _STREAM_AGENT_CACHE[name]


async def query_stream_graph(req: Any):
    """
    åŸºäºLangGraphçš„æµå¼æŸ¥è¯¢æ¥å£

    Args:
        req: æŸ¥è¯¢è¯·æ±‚

    Returns:
        EventSourceResponse: SSEæµå¼å“åº”
    """
    if not req.query.strip():
        raise HTTPException(status_code=400, detail="query ä¸èƒ½ä¸ºç©º")

    async def event_generator():
        """SSEäº‹ä»¶ç”Ÿæˆå™¨"""
        try:
            # è½¬æ¢èŠå¤©å†å²æ ¼å¼
            chat_history = []
            if req.chat_history:
                for msg in req.chat_history:
                    role = msg.get("role", "")
                    content = msg.get("content", "")
                    if role == "user":
                        chat_history.append(("human", content))
                    elif role == "assistant":
                        chat_history.append(("ai", content))

            print(f"\n[ğŸŒŠ Stream] å¼€å§‹æµå¼å¤„ç†")
            print(f"[ğŸŒŠ Stream] æŸ¥è¯¢: {req.query[:50]}...")
            print(f"[ğŸŒŠ Stream] æ¨¡å¼: {req.agent_name or 'auto'}")
            print(f"[ğŸŒŠ Stream] å†å²è®°å½•: {len(chat_history)} æ¡")

            # å°† agent_name è½¬æ¢ä¸º mode_hintï¼ˆç”¨äºéšå¼æ„å›¾è¯†åˆ«ï¼‰
            mode_hint = None
            if req.agent_name == "research_agent" or req.agent_name == "research":
                mode_hint = "research"
            elif req.agent_name == "fortune_agent" or req.agent_name == "fortune":
                mode_hint = "fortune"

            # éšå¼æ„å›¾è¯†åˆ«å¾—åˆ°ç›®æ ‡ agent
            routing = classify_and_route(req.query, mode_hint)
            agent_name = routing.get("agent_name")
            executor = _get_stream_executor(agent_name)

            # å‡†å¤‡è°ƒç”¨å‚æ•°ï¼ˆchat_history å¿…é¡»æ˜¯åˆ—è¡¨ï¼‰
            invoke_params = {
                "input": req.query,
                "context": "",
                "chat_history": chat_history,
            }

            accumulated_output = ""
            has_stream = hasattr(executor, "stream") and callable(executor.stream)

            if has_stream:
                for chunk in executor.stream(invoke_params):
                    if isinstance(chunk, dict) and "output" in chunk:
                        current_output = chunk.get("output", "")
                        if current_output and current_output.startswith(accumulated_output):
                            delta = current_output[len(accumulated_output):]
                            if delta:
                                import json
                                yield {
                                    "event": "message",
                                    "data": json.dumps({
                                        "type": "delta",
                                        "data": delta
                                    }, ensure_ascii=False)
                                }
                                accumulated_output = current_output
            else:
                # è‹¥æ—  stream èƒ½åŠ›ï¼Œå›é€€ä¸ºä¸€æ¬¡æ€§è¾“å‡º
                result = executor.invoke(invoke_params)
                output = result.get("output", "") if isinstance(result, dict) else str(result)
                if output:
                    import json
                    yield {
                        "event": "message",
                        "data": json.dumps({
                            "type": "delta",
                            "data": output
                        }, ensure_ascii=False)
                    }

            # å‘é€å®Œæˆä¿¡å·
            import json

            yield {
                "event": "message",
                "data": json.dumps({"type": "done"}, ensure_ascii=False)
            }

            print(f"[âœ… Stream] æµå¼å¤„ç†å®Œæˆ")

        except Exception as e:
            print(f"[âŒ Stream] é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

            # å‘é€é”™è¯¯äº‹ä»¶
            import json

            yield {
                "event": "message",
                "data": json.dumps({
                    "type": "error",
                    "message": f"å¤„ç†å¤±è´¥: {str(e)}"
                }, ensure_ascii=False)
            }

    return EventSourceResponse(
        event_generator(),
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        },
    )


async def query_sync_graph(req: Any) -> Dict[str, Any]:
    """
    åŸºäºLangGraphçš„åŒæ­¥æŸ¥è¯¢æ¥å£

    Args:
        req: æŸ¥è¯¢è¯·æ±‚

    Returns:
        Dict: åŒ…å«ç­”æ¡ˆçš„å­—å…¸
    """
    if not req.query.strip():
        raise HTTPException(status_code=400, detail="query ä¸èƒ½ä¸ºç©º")

    try:
        # è½¬æ¢èŠå¤©å†å²æ ¼å¼
        chat_history = []
        if req.chat_history:
            for msg in req.chat_history:
                role = msg.get("role", "")
                content = msg.get("content", "")
                if role == "user":
                    chat_history.append(("human", content))
                elif role == "assistant":
                    chat_history.append(("ai", content))

        print(f"\n[ğŸ”„ Graph Sync] å¼€å§‹åŒæ­¥å¤„ç†")
        print(f"[ğŸ”„ Graph Sync] æŸ¥è¯¢: {req.query[:50]}...")

        # åŒæ­¥è°ƒç”¨Graph
        result = await run_graph(
            query=req.query,
            chat_history=chat_history,
            mode_hint=None,
        )

        answer = result.get("final_answer", "") or result.get("output", "")

        print(f"[âœ… Graph Sync] åŒæ­¥å¤„ç†å®Œæˆ")

        return {
            "answer": answer,
            "route": result.get("route", ""),
            "metadata": result.get("metadata", {}),
        }

    except Exception as e:
        print(f"[âŒ Graph Sync] é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")
